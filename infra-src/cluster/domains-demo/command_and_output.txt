# This command can take up to 20 minutes to create the whole cluster
$ eksctl create cluster -f domains-demo.yaml
[ℹ]  eksctl version 0.11.1
[ℹ]  using region eu-west-1
[ℹ]  setting availability zones to [eu-west-1c eu-west-1a eu-west-1b]
[ℹ]  subnets for eu-west-1c - public:192.168.0.0/19 private:192.168.96.0/19
[ℹ]  subnets for eu-west-1a - public:192.168.32.0/19 private:192.168.128.0/19
[ℹ]  subnets for eu-west-1b - public:192.168.64.0/19 private:192.168.160.0/19
[ℹ]  nodegroup "ng-1-spot-instances" will use "ami-059c6874350e63ca9" [AmazonLinux2/1.14]
[ℹ]  using Kubernetes version 1.14
[ℹ]  creating EKS cluster "domains-demo" in "eu-west-1" region with managed nodes and un-managed nodes
[ℹ]  2 nodegroups (ng-0-on-demand, ng-1-spot-instances) were included (based on the include/exclude rules)
[ℹ]  will create a CloudFormation stack for cluster itself and 1 nodegroup stack(s)
[ℹ]  will create a CloudFormation stack for cluster itself and 1 managed nodegroup stack(s)
[ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=eu-west-1 --cluster=domains-demo'
[ℹ]  CloudWatch logging will not be enabled for cluster "domains-demo" in "eu-west-1"
[ℹ]  you can enable it with 'eksctl utils update-cluster-logging --region=eu-west-1 --cluster=domains-demo'
[ℹ]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster "domains-demo" in "eu-west-1"
[ℹ]  2 sequential tasks: { create cluster control plane "domains-demo", 2 parallel sub-tasks: { create nodegroup "ng-1-spot-instances", create managed nodegroup "ng-0-on-demand" } }
[ℹ]  building cluster stack "eksctl-domains-demo-cluster"
[ℹ]  deploying stack "eksctl-domains-demo-cluster"
[ℹ]  building managed nodegroup stack "eksctl-domains-demo-nodegroup-ng-0-on-demand"
[ℹ]  building nodegroup stack "eksctl-domains-demo-nodegroup-ng-1-spot-instances"
[ℹ]  deploying stack "eksctl-domains-demo-nodegroup-ng-0-on-demand"
[ℹ]  deploying stack "eksctl-domains-demo-nodegroup-ng-1-spot-instances"
[✔]  all EKS cluster resources for "domains-demo" have been created
[✔]  saved kubeconfig as "/Users/jwimsingues/.kube/config"
[ℹ]  adding identity "arn:aws:iam::335727272277:role/eksctl-domains-demo-nodegroup-ng-NodeInstanceRole-100734QQIJS7" to auth ConfigMap
[ℹ]  nodegroup "ng-1-spot-instances" has 0 node(s)
[ℹ]  waiting for at least 1 node(s) to become ready in "ng-1-spot-instances"
[ℹ]  nodegroup "ng-1-spot-instances" has 1 node(s)
[ℹ]  node "ip-192-168-38-133.eu-west-1.compute.internal" is ready
[ℹ]  nodegroup "ng-0-on-demand" has 1 node(s)
[ℹ]  node "ip-192-168-28-27.eu-west-1.compute.internal" is ready
[ℹ]  waiting for at least 1 node(s) to become ready in "ng-0-on-demand"
[ℹ]  nodegroup "ng-0-on-demand" has 1 node(s)
[ℹ]  node "ip-192-168-28-27.eu-west-1.compute.internal" is ready
[ℹ]  kubectl command should work with "/Users/jwimsingues/.kube/config", try 'kubectl get nodes'
[✔]  EKS cluster "domains-demo" in "eu-west-1" region is ready
# Check cluster creation
$ eksctl get clusters
NAME		REGION
domains-demo	eu-west-1
# Check nodegroup creation
$ eksctl get nodegroup --cluster domains-demo
CLUSTER		NODEGROUP		CREATED			MIN SIZE	MAX SIZE	DESIRED CAPACITY	INSTANCE TYPE	IMAGE ID
domains-demo	ng-0-on-demand		2020-01-03T13:41:18Z	1		3		1			t3.medium
domains-demo	ng-1-spot-instances	2020-01-03T13:41:18Z	1		10		0			t3.small	ami-059c6874350e63ca9